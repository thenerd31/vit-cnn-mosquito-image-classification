{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b94e4558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration TheNoob3131--mosquito-data-bf1f4d4d5f427763\n",
      "Reusing dataset csv (C:\\Users\\Aswin.Surya24\\.cache\\huggingface\\datasets\\TheNoob3131___csv\\TheNoob3131--mosquito-data-bf1f4d4d5f427763\\0.0.0\\51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cae28c886a449bc97160f5054b3bad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('TheNoob3131/mosquito-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cbecab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'].features['Photo']\n",
    "from datasets import Image\n",
    "dataset = dataset.cast_column('Photo', Image(decode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9871c5af",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Column (Classification) not in table columns (['MHM Id', 'labels', 'Container', 'Userid', 'latitude', 'longitude', 'date', 'Photo']).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_encode_column\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mClassification\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mrename_column(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClassification\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\datasets\\dataset_dict.py:448\u001b[0m, in \u001b[0;36mDatasetDict.class_encode_column\u001b[1;34m(self, column, include_nulls)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;124;03m\"\"\"Casts the given column as :obj:``datasets.features.ClassLabel`` and updates the tables.\u001b[39;00m\n\u001b[0;32m    422\u001b[0m \n\u001b[0;32m    423\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_values_type()\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m--> 448\u001b[0m     {k: dataset\u001b[38;5;241m.\u001b[39mclass_encode_column(column\u001b[38;5;241m=\u001b[39mcolumn, include_nulls\u001b[38;5;241m=\u001b[39minclude_nulls) \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    449\u001b[0m )\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\datasets\\dataset_dict.py:448\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;124;03m\"\"\"Casts the given column as :obj:``datasets.features.ClassLabel`` and updates the tables.\u001b[39;00m\n\u001b[0;32m    422\u001b[0m \n\u001b[0;32m    423\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_values_type()\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m--> 448\u001b[0m     {k: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_encode_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_nulls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_nulls\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    449\u001b[0m )\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py:1384\u001b[0m, in \u001b[0;36mDataset.class_encode_column\u001b[1;34m(self, column, include_nulls)\u001b[0m\n\u001b[0;32m   1382\u001b[0m \u001b[38;5;66;03m# Sanity checks\u001b[39;00m\n\u001b[0;32m   1383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m column \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names:\n\u001b[1;32m-> 1384\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) not in table columns (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1385\u001b[0m src_feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures[column]\n\u001b[0;32m   1386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(src_feat, Value):\n",
      "\u001b[1;31mValueError\u001b[0m: Column (Classification) not in table columns (['MHM Id', 'labels', 'Container', 'Userid', 'latitude', 'longitude', 'date', 'Photo'])."
     ]
    }
   ],
   "source": [
    "dataset = dataset.class_encode_column('Classification')\n",
    "dataset = dataset.rename_column('Classification', 'labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fdfa807c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aedes', 'culex', 'neither']\n"
     ]
    }
   ],
   "source": [
    "labels = dataset[\"train\"].features[\"labels\"].names\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3f0a934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'aedes', 1: 'culex', 2: 'neither'}\n"
     ]
    }
   ],
   "source": [
    "id2label = {k:v for k,v in enumerate(labels)}\n",
    "label2id = {v:k for k,v in enumerate(labels)}\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c29818e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading feature extractor configuration file https://huggingface.co/microsoft/cvt-13/resolve/main/preprocessor_config.json from cache at C:\\Users\\Aswin.Surya24/.cache\\huggingface\\transformers\\1745fd90e07f290b67705e8a801213fa61b4fab9640f05b898ad57c360cf4314.37be7274d6b5860aee104bb1fbaeb0722fec3850a85bb2557ae9491f17f89433\n",
      "Feature extractor ConvNextFeatureExtractor {\n",
      "  \"crop_pct\": 0.875,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"ConvNextFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.485,\n",
      "    0.456,\n",
      "    0.406\n",
      "  ],\n",
      "  \"image_std\": [\n",
      "    0.229,\n",
      "    0.224,\n",
      "    0.225\n",
      "  ],\n",
      "  \"resample\": 3,\n",
      "  \"size\": 224\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor, CvtForImageClassification\n",
    "model_name_or_path = 'microsoft/cvt-13'\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "358eb17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "\n",
    "transform = Compose(\n",
    "    [\n",
    "     RandomResizedCrop(feature_extractor.size),\n",
    "     RandomHorizontalFlip(),\n",
    "     ToTensor(),\n",
    "     normalize\n",
    "    ]\n",
    ")\n",
    "\n",
    "def train_transforms(examples):\n",
    "  examples[\"pixel_values\"] = [transform(image.convert(\"RGB\")) for image in examples[\"Photo\"]]\n",
    "\n",
    "  return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0cd2ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_ds = dataset.with_transform(train_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3aecc96d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Image(decode=True, id=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_ds['train'].features['Photo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "613bcf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"labels\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "dataloader = DataLoader(prepared_ds[\"train\"], collate_fn=collate_fn, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "54320649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([4, 3, 224, 224])\n",
      "labels torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dataloader))\n",
    "for k,v in batch.items():\n",
    "  if isinstance(v, torch.Tensor):\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4f7faea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Image(decode=True, id=None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_ds['train'].features['Photo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1f914bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/microsoft/cvt-13/resolve/main/config.json from cache at C:\\Users\\Aswin.Surya24/.cache\\huggingface\\transformers\\4ef5c630651d59551ff5a0071e4bc20b89c24bfb1b2af58442d4b0cb59b79f89.267c6ccda346362800e3a22914da775054354a112accbeff2a01703d0fa98432\n",
      "Model config CvtConfig {\n",
      "  \"architectures\": [\n",
      "    \"CvtForImageClassification\"\n",
      "  ],\n",
      "  \"attention_drop_rate\": [\n",
      "    0.0,\n",
      "    0.0,\n",
      "    0.0\n",
      "  ],\n",
      "  \"cls_token\": [\n",
      "    false,\n",
      "    false,\n",
      "    true\n",
      "  ],\n",
      "  \"depth\": [\n",
      "    1,\n",
      "    2,\n",
      "    10\n",
      "  ],\n",
      "  \"drop_path_rate\": [\n",
      "    0.0,\n",
      "    0.0,\n",
      "    0.1\n",
      "  ],\n",
      "  \"drop_rate\": [\n",
      "    0.0,\n",
      "    0.0,\n",
      "    0.0\n",
      "  ],\n",
      "  \"embed_dim\": [\n",
      "    64,\n",
      "    192,\n",
      "    384\n",
      "  ],\n",
      "  \"id2label\": {\n",
      "    \"0\": \"aedes\",\n",
      "    \"1\": \"culex\",\n",
      "    \"2\": \"neither\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"kernel_qkv\": [\n",
      "    3,\n",
      "    3,\n",
      "    3\n",
      "  ],\n",
      "  \"label2id\": {\n",
      "    \"aedes\": 0,\n",
      "    \"culex\": 1,\n",
      "    \"neither\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mlp_ratio\": [\n",
      "    4.0,\n",
      "    4.0,\n",
      "    4.0\n",
      "  ],\n",
      "  \"model_type\": \"cvt\",\n",
      "  \"num_channels\": 3,\n",
      "  \"num_heads\": [\n",
      "    1,\n",
      "    3,\n",
      "    6\n",
      "  ],\n",
      "  \"num_stages\": 3,\n",
      "  \"padding_kv\": [\n",
      "    1,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"padding_q\": [\n",
      "    1,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"patch_padding\": [\n",
      "    2,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"patch_sizes\": [\n",
      "    7,\n",
      "    3,\n",
      "    3\n",
      "  ],\n",
      "  \"patch_stride\": [\n",
      "    4,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"pos_embed\": [\n",
      "    false,\n",
      "    false,\n",
      "    false\n",
      "  ],\n",
      "  \"qkv_bias\": [\n",
      "    true,\n",
      "    true,\n",
      "    true\n",
      "  ],\n",
      "  \"qkv_projection_method\": [\n",
      "    \"dw_bn\",\n",
      "    \"dw_bn\",\n",
      "    \"dw_bn\"\n",
      "  ],\n",
      "  \"stride_kv\": [\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"stride_q\": [\n",
      "    1,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.20.1\"\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/microsoft/cvt-13/resolve/main/pytorch_model.bin from cache at C:\\Users\\Aswin.Surya24/.cache\\huggingface\\transformers\\cd5faff4875945b270d8cee934a24ded1aa971d179aa1354fa1b97ed30a53986.96069fab8bfe8637f34854f2523293936778cc80b1b028df03751a9bff69972a\n",
      "All model checkpoint weights were used when initializing CvtForImageClassification.\n",
      "\n",
      "Some weights of CvtForImageClassification were not initialized from the model checkpoint at microsoft/cvt-13 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 384]) in the checkpoint and torch.Size([3, 384]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CvtForImageClassification(\n",
       "  (cvt): CvtModel(\n",
       "    (encoder): CvtEncoder(\n",
       "      (stages): ModuleList(\n",
       "        (0): CvtStage(\n",
       "          (embedding): CvtEmbeddings(\n",
       "            (convolution_embeddings): CvtConvEmbeddings(\n",
       "              (projection): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))\n",
       "              (normalization): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layers): Sequential(\n",
       "            (0): CvtLayer(\n",
       "              (attention): CvtAttention(\n",
       "                (attention): CvtSelfAttention(\n",
       "                  (convolution_projection_query): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "                      (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (convolution_projection_key): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "                      (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (convolution_projection_value): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "                      (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (projection_query): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (projection_key): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (projection_value): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): CvtSelfOutput(\n",
       "                  (dense): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): CvtIntermediate(\n",
       "                (dense): Linear(in_features=64, out_features=256, bias=True)\n",
       "                (activation): GELU(approximate=none)\n",
       "              )\n",
       "              (output): CvtOutput(\n",
       "                (dense): Linear(in_features=256, out_features=64, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (layernorm_before): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "              (layernorm_after): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): CvtStage(\n",
       "          (embedding): CvtEmbeddings(\n",
       "            (convolution_embeddings): CvtConvEmbeddings(\n",
       "              (projection): Conv2d(64, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "              (normalization): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layers): Sequential(\n",
       "            (0): CvtLayer(\n",
       "              (attention): CvtAttention(\n",
       "                (attention): CvtSelfAttention(\n",
       "                  (convolution_projection_query): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "                      (normalization): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (convolution_projection_key): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "                      (normalization): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (convolution_projection_value): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "                      (normalization): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (projection_query): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (projection_key): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (projection_value): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): CvtSelfOutput(\n",
       "                  (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): CvtIntermediate(\n",
       "                (dense): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (activation): GELU(approximate=none)\n",
       "              )\n",
       "              (output): CvtOutput(\n",
       "                (dense): Linear(in_features=768, out_features=192, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (1): CvtLayer(\n",
       "              (attention): CvtAttention(\n",
       "                (attention): CvtSelfAttention(\n",
       "                  (convolution_projection_query): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "                      (normalization): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (convolution_projection_key): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "                      (normalization): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (convolution_projection_value): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "                      (normalization): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (projection_query): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (projection_key): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (projection_value): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): CvtSelfOutput(\n",
       "                  (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): CvtIntermediate(\n",
       "                (dense): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (activation): GELU(approximate=none)\n",
       "              )\n",
       "              (output): CvtOutput(\n",
       "                (dense): Linear(in_features=768, out_features=192, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): CvtStage(\n",
       "          (embedding): CvtEmbeddings(\n",
       "            (convolution_embeddings): CvtConvEmbeddings(\n",
       "              (projection): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "              (normalization): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layers): Sequential(\n",
       "            (0): CvtLayer(\n",
       "              (attention): CvtAttention(\n",
       "                (attention): CvtSelfAttention(\n",
       "                  (convolution_projection_query): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                      (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (convolution_projection_key): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
       "                      (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (convolution_projection_value): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
       "                      (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (projection_query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (projection_key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (projection_value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): CvtSelfOutput(\n",
       "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): CvtIntermediate(\n",
       "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (activation): GELU(approximate=none)\n",
       "              )\n",
       "              (output): CvtOutput(\n",
       "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path): CvtDropPath()\n",
       "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (1): CvtLayer(\n",
       "              (attention): CvtAttention(\n",
       "                (attention): CvtSelfAttention(\n",
       "                  (convolution_projection_query): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                      (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (convolution_projection_key): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
       "                      (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (convolution_projection_value): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
       "                      (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (projection_query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (projection_key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (projection_value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): CvtSelfOutput(\n",
       "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): CvtIntermediate(\n",
       "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (activation): GELU(approximate=none)\n",
       "              )\n",
       "              (output): CvtOutput(\n",
       "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path): CvtDropPath()\n",
       "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (2): CvtLayer(\n",
       "              (attention): CvtAttention(\n",
       "                (attention): CvtSelfAttention(\n",
       "                  (convolution_projection_query): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                      (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (convolution_projection_key): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
       "                      (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (convolution_projection_value): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
       "                      (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (projection_query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (projection_key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (projection_value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): CvtSelfOutput(\n",
       "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): CvtIntermediate(\n",
       "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (activation): GELU(approximate=none)\n",
       "              )\n",
       "              (output): CvtOutput(\n",
       "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path): CvtDropPath()\n",
       "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (3): CvtLayer(\n",
       "              (attention): CvtAttention(\n",
       "                (attention): CvtSelfAttention(\n",
       "                  (convolution_projection_query): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                      (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (convolution_projection_key): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
       "                      (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (convolution_projection_value): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
       "                      (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (projection_query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (projection_key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (projection_value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): CvtSelfOutput(\n",
       "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): CvtIntermediate(\n",
       "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (activation): GELU(approximate=none)\n",
       "              )\n",
       "              (output): CvtOutput(\n",
       "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path): CvtDropPath()\n",
       "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (4): CvtLayer(\n",
       "              (attention): CvtAttention(\n",
       "                (attention): CvtSelfAttention(\n",
       "                  (convolution_projection_query): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                      (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (convolution_projection_key): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
       "                      (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (convolution_projection_value): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
       "                      (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (projection_query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (projection_key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (projection_value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): CvtSelfOutput(\n",
       "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): CvtIntermediate(\n",
       "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (activation): GELU(approximate=none)\n",
       "              )\n",
       "              (output): CvtOutput(\n",
       "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path): CvtDropPath()\n",
       "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (5): CvtLayer(\n",
       "              (attention): CvtAttention(\n",
       "                (attention): CvtSelfAttention(\n",
       "                  (convolution_projection_query): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                      (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (convolution_projection_key): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
       "                      (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (convolution_projection_value): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
       "                      (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (projection_query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (projection_key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (projection_value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): CvtSelfOutput(\n",
       "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): CvtIntermediate(\n",
       "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (activation): GELU(approximate=none)\n",
       "              )\n",
       "              (output): CvtOutput(\n",
       "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path): CvtDropPath()\n",
       "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (6): CvtLayer(\n",
       "              (attention): CvtAttention(\n",
       "                (attention): CvtSelfAttention(\n",
       "                  (convolution_projection_query): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                      (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (convolution_projection_key): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
       "                      (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (convolution_projection_value): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
       "                      (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (projection_query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (projection_key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (projection_value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): CvtSelfOutput(\n",
       "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): CvtIntermediate(\n",
       "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (activation): GELU(approximate=none)\n",
       "              )\n",
       "              (output): CvtOutput(\n",
       "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path): CvtDropPath()\n",
       "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (7): CvtLayer(\n",
       "              (attention): CvtAttention(\n",
       "                (attention): CvtSelfAttention(\n",
       "                  (convolution_projection_query): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                      (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (convolution_projection_key): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
       "                      (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (convolution_projection_value): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
       "                      (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (projection_query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (projection_key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (projection_value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): CvtSelfOutput(\n",
       "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): CvtIntermediate(\n",
       "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (activation): GELU(approximate=none)\n",
       "              )\n",
       "              (output): CvtOutput(\n",
       "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path): CvtDropPath()\n",
       "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (8): CvtLayer(\n",
       "              (attention): CvtAttention(\n",
       "                (attention): CvtSelfAttention(\n",
       "                  (convolution_projection_query): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                      (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (convolution_projection_key): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
       "                      (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (convolution_projection_value): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
       "                      (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (projection_query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (projection_key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (projection_value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): CvtSelfOutput(\n",
       "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): CvtIntermediate(\n",
       "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (activation): GELU(approximate=none)\n",
       "              )\n",
       "              (output): CvtOutput(\n",
       "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path): CvtDropPath()\n",
       "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (9): CvtLayer(\n",
       "              (attention): CvtAttention(\n",
       "                (attention): CvtSelfAttention(\n",
       "                  (convolution_projection_query): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "                      (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (convolution_projection_key): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
       "                      (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (convolution_projection_value): CvtSelfAttentionProjection(\n",
       "                    (convolution_projection): CvtSelfAttentionConvProjection(\n",
       "                      (convolution): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
       "                      (normalization): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                    )\n",
       "                    (linear_projection): CvtSelfAttentionLinearProjection()\n",
       "                  )\n",
       "                  (projection_query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (projection_key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (projection_value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): CvtSelfOutput(\n",
       "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): CvtIntermediate(\n",
       "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (activation): GELU(approximate=none)\n",
       "              )\n",
       "              (output): CvtOutput(\n",
       "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path): CvtDropPath()\n",
       "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  (classifier): Linear(in_features=384, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import CvtForImageClassification\n",
    "labels = dataset['train'].features['labels'].names\n",
    "#print(len(labels))\n",
    "model = CvtForImageClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6d0b1e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    metric1 = load_metric(\"precision\")\n",
    "    metric2 = load_metric(\"recall\")\n",
    "    metric3 = load_metric(\"f1\")\n",
    "    metric4 = load_metric(\"accuracy\")\n",
    "  \n",
    "\n",
    "    \n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision = metric1.compute(predictions=predictions, references=labels, average=\"weighted\")[\"precision\"]\n",
    "    recall = metric2.compute(predictions=predictions, references=labels, average=\"weighted\")[\"recall\"]\n",
    "    f1 = metric3.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"]\n",
    "    accuracy = metric4.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1, \"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d6d5ed17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./cvt-13-mosquito\",\n",
    "  per_device_train_batch_size=16,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=4,\n",
    "  save_steps=100,\n",
    "  eval_steps=100,\n",
    "  logging_steps=10,\n",
    "  learning_rate=2e-4,\n",
    "  save_total_limit=2,\n",
    "  remove_unused_columns=False,\n",
    "  push_to_hub=False,\n",
    "  report_to='tensorboard',\n",
    "  load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bdb666dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepared_ds[\"train\"],\n",
    "    eval_dataset=prepared_ds[\"test\"],\n",
    "    tokenizer=feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3dcb93a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aswin.Surya24\\Anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 7107\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1780\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1780' max='1780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1780/1780 13:42:56, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.912600</td>\n",
       "      <td>0.985329</td>\n",
       "      <td>0.563211</td>\n",
       "      <td>0.588834</td>\n",
       "      <td>0.563162</td>\n",
       "      <td>0.588834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.756900</td>\n",
       "      <td>0.821403</td>\n",
       "      <td>0.594216</td>\n",
       "      <td>0.616458</td>\n",
       "      <td>0.602963</td>\n",
       "      <td>0.616458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.816100</td>\n",
       "      <td>0.838465</td>\n",
       "      <td>0.619309</td>\n",
       "      <td>0.627799</td>\n",
       "      <td>0.541277</td>\n",
       "      <td>0.627799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.834100</td>\n",
       "      <td>0.944966</td>\n",
       "      <td>0.595319</td>\n",
       "      <td>0.560919</td>\n",
       "      <td>0.560559</td>\n",
       "      <td>0.560919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.805400</td>\n",
       "      <td>0.845369</td>\n",
       "      <td>0.623043</td>\n",
       "      <td>0.617621</td>\n",
       "      <td>0.578288</td>\n",
       "      <td>0.617621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.794500</td>\n",
       "      <td>0.822950</td>\n",
       "      <td>0.616606</td>\n",
       "      <td>0.640593</td>\n",
       "      <td>0.621026</td>\n",
       "      <td>0.640593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.799400</td>\n",
       "      <td>0.827959</td>\n",
       "      <td>0.595163</td>\n",
       "      <td>0.614714</td>\n",
       "      <td>0.600080</td>\n",
       "      <td>0.614714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.751300</td>\n",
       "      <td>0.892511</td>\n",
       "      <td>0.616770</td>\n",
       "      <td>0.605118</td>\n",
       "      <td>0.595347</td>\n",
       "      <td>0.605118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.670100</td>\n",
       "      <td>0.866128</td>\n",
       "      <td>0.607960</td>\n",
       "      <td>0.590288</td>\n",
       "      <td>0.598668</td>\n",
       "      <td>0.590288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.657900</td>\n",
       "      <td>0.790230</td>\n",
       "      <td>0.624717</td>\n",
       "      <td>0.636813</td>\n",
       "      <td>0.618272</td>\n",
       "      <td>0.636813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.790600</td>\n",
       "      <td>0.823016</td>\n",
       "      <td>0.640802</td>\n",
       "      <td>0.620820</td>\n",
       "      <td>0.627849</td>\n",
       "      <td>0.620820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.651600</td>\n",
       "      <td>0.868462</td>\n",
       "      <td>0.639724</td>\n",
       "      <td>0.618785</td>\n",
       "      <td>0.604558</td>\n",
       "      <td>0.618785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.768300</td>\n",
       "      <td>0.837883</td>\n",
       "      <td>0.649327</td>\n",
       "      <td>0.635068</td>\n",
       "      <td>0.618692</td>\n",
       "      <td>0.635068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.589900</td>\n",
       "      <td>0.863885</td>\n",
       "      <td>0.634753</td>\n",
       "      <td>0.618494</td>\n",
       "      <td>0.611920</td>\n",
       "      <td>0.618494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.613200</td>\n",
       "      <td>0.840668</td>\n",
       "      <td>0.623543</td>\n",
       "      <td>0.620529</td>\n",
       "      <td>0.617824</td>\n",
       "      <td>0.620529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.629800</td>\n",
       "      <td>0.833048</td>\n",
       "      <td>0.658395</td>\n",
       "      <td>0.643792</td>\n",
       "      <td>0.635098</td>\n",
       "      <td>0.643792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.601600</td>\n",
       "      <td>0.853770</td>\n",
       "      <td>0.646077</td>\n",
       "      <td>0.616749</td>\n",
       "      <td>0.624391</td>\n",
       "      <td>0.616749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3439\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./cvt-13-mosquito\\checkpoint-100\n",
      "Configuration saved in ./cvt-13-mosquito\\checkpoint-100\\config.json\n",
      "Model weights saved in ./cvt-13-mosquito\\checkpoint-100\\pytorch_model.bin\n",
      "Feature extractor saved in ./cvt-13-mosquito\\checkpoint-100\\preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3439\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./cvt-13-mosquito\\checkpoint-200\n",
      "Configuration saved in ./cvt-13-mosquito\\checkpoint-200\\config.json\n",
      "Model weights saved in ./cvt-13-mosquito\\checkpoint-200\\pytorch_model.bin\n",
      "Feature extractor saved in ./cvt-13-mosquito\\checkpoint-200\\preprocessor_config.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3439\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./cvt-13-mosquito\\checkpoint-300\n",
      "Configuration saved in ./cvt-13-mosquito\\checkpoint-300\\config.json\n",
      "Model weights saved in ./cvt-13-mosquito\\checkpoint-300\\pytorch_model.bin\n",
      "Feature extractor saved in ./cvt-13-mosquito\\checkpoint-300\\preprocessor_config.json\n",
      "Deleting older checkpoint [cvt-13-mosquito\\checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3439\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./cvt-13-mosquito\\checkpoint-400\n",
      "Configuration saved in ./cvt-13-mosquito\\checkpoint-400\\config.json\n",
      "Model weights saved in ./cvt-13-mosquito\\checkpoint-400\\pytorch_model.bin\n",
      "Feature extractor saved in ./cvt-13-mosquito\\checkpoint-400\\preprocessor_config.json\n",
      "Deleting older checkpoint [cvt-13-mosquito\\checkpoint-300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3439\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./cvt-13-mosquito\\checkpoint-500\n",
      "Configuration saved in ./cvt-13-mosquito\\checkpoint-500\\config.json\n",
      "Model weights saved in ./cvt-13-mosquito\\checkpoint-500\\pytorch_model.bin\n",
      "Feature extractor saved in ./cvt-13-mosquito\\checkpoint-500\\preprocessor_config.json\n",
      "Deleting older checkpoint [cvt-13-mosquito\\checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3439\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./cvt-13-mosquito\\checkpoint-600\n",
      "Configuration saved in ./cvt-13-mosquito\\checkpoint-600\\config.json\n",
      "Model weights saved in ./cvt-13-mosquito\\checkpoint-600\\pytorch_model.bin\n",
      "Feature extractor saved in ./cvt-13-mosquito\\checkpoint-600\\preprocessor_config.json\n",
      "Deleting older checkpoint [cvt-13-mosquito\\checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3439\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./cvt-13-mosquito\\checkpoint-700\n",
      "Configuration saved in ./cvt-13-mosquito\\checkpoint-700\\config.json\n",
      "Model weights saved in ./cvt-13-mosquito\\checkpoint-700\\pytorch_model.bin\n",
      "Feature extractor saved in ./cvt-13-mosquito\\checkpoint-700\\preprocessor_config.json\n",
      "Deleting older checkpoint [cvt-13-mosquito\\checkpoint-600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3439\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./cvt-13-mosquito\\checkpoint-800\n",
      "Configuration saved in ./cvt-13-mosquito\\checkpoint-800\\config.json\n",
      "Model weights saved in ./cvt-13-mosquito\\checkpoint-800\\pytorch_model.bin\n",
      "Feature extractor saved in ./cvt-13-mosquito\\checkpoint-800\\preprocessor_config.json\n",
      "Deleting older checkpoint [cvt-13-mosquito\\checkpoint-700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3439\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./cvt-13-mosquito\\checkpoint-900\n",
      "Configuration saved in ./cvt-13-mosquito\\checkpoint-900\\config.json\n",
      "Model weights saved in ./cvt-13-mosquito\\checkpoint-900\\pytorch_model.bin\n",
      "Feature extractor saved in ./cvt-13-mosquito\\checkpoint-900\\preprocessor_config.json\n",
      "Deleting older checkpoint [cvt-13-mosquito\\checkpoint-800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3439\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./cvt-13-mosquito\\checkpoint-1000\n",
      "Configuration saved in ./cvt-13-mosquito\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./cvt-13-mosquito\\checkpoint-1000\\pytorch_model.bin\n",
      "Feature extractor saved in ./cvt-13-mosquito\\checkpoint-1000\\preprocessor_config.json\n",
      "Deleting older checkpoint [cvt-13-mosquito\\checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3439\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./cvt-13-mosquito\\checkpoint-1100\n",
      "Configuration saved in ./cvt-13-mosquito\\checkpoint-1100\\config.json\n",
      "Model weights saved in ./cvt-13-mosquito\\checkpoint-1100\\pytorch_model.bin\n",
      "Feature extractor saved in ./cvt-13-mosquito\\checkpoint-1100\\preprocessor_config.json\n",
      "Deleting older checkpoint [cvt-13-mosquito\\checkpoint-900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3439\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./cvt-13-mosquito\\checkpoint-1200\n",
      "Configuration saved in ./cvt-13-mosquito\\checkpoint-1200\\config.json\n",
      "Model weights saved in ./cvt-13-mosquito\\checkpoint-1200\\pytorch_model.bin\n",
      "Feature extractor saved in ./cvt-13-mosquito\\checkpoint-1200\\preprocessor_config.json\n",
      "Deleting older checkpoint [cvt-13-mosquito\\checkpoint-1100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3439\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./cvt-13-mosquito\\checkpoint-1300\n",
      "Configuration saved in ./cvt-13-mosquito\\checkpoint-1300\\config.json\n",
      "Model weights saved in ./cvt-13-mosquito\\checkpoint-1300\\pytorch_model.bin\n",
      "Feature extractor saved in ./cvt-13-mosquito\\checkpoint-1300\\preprocessor_config.json\n",
      "Deleting older checkpoint [cvt-13-mosquito\\checkpoint-1200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3439\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./cvt-13-mosquito\\checkpoint-1400\n",
      "Configuration saved in ./cvt-13-mosquito\\checkpoint-1400\\config.json\n",
      "Model weights saved in ./cvt-13-mosquito\\checkpoint-1400\\pytorch_model.bin\n",
      "Feature extractor saved in ./cvt-13-mosquito\\checkpoint-1400\\preprocessor_config.json\n",
      "Deleting older checkpoint [cvt-13-mosquito\\checkpoint-1300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3439\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./cvt-13-mosquito\\checkpoint-1500\n",
      "Configuration saved in ./cvt-13-mosquito\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./cvt-13-mosquito\\checkpoint-1500\\pytorch_model.bin\n",
      "Feature extractor saved in ./cvt-13-mosquito\\checkpoint-1500\\preprocessor_config.json\n",
      "Deleting older checkpoint [cvt-13-mosquito\\checkpoint-1400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3439\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./cvt-13-mosquito\\checkpoint-1600\n",
      "Configuration saved in ./cvt-13-mosquito\\checkpoint-1600\\config.json\n",
      "Model weights saved in ./cvt-13-mosquito\\checkpoint-1600\\pytorch_model.bin\n",
      "Feature extractor saved in ./cvt-13-mosquito\\checkpoint-1600\\preprocessor_config.json\n",
      "Deleting older checkpoint [cvt-13-mosquito\\checkpoint-1500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3439\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./cvt-13-mosquito\\checkpoint-1700\n",
      "Configuration saved in ./cvt-13-mosquito\\checkpoint-1700\\config.json\n",
      "Model weights saved in ./cvt-13-mosquito\\checkpoint-1700\\pytorch_model.bin\n",
      "Feature extractor saved in ./cvt-13-mosquito\\checkpoint-1700\\preprocessor_config.json\n",
      "Deleting older checkpoint [cvt-13-mosquito\\checkpoint-1600] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./cvt-13-mosquito\\checkpoint-1000 (score: 0.7902304530143738).\n",
      "Saving model checkpoint to ./cvt-13-mosquito\n",
      "Configuration saved in ./cvt-13-mosquito\\config.json\n",
      "Model weights saved in ./cvt-13-mosquito\\pytorch_model.bin\n",
      "Feature extractor saved in ./cvt-13-mosquito\\preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =         4.0\n",
      "  total_flos               = 469000242GF\n",
      "  train_loss               =      0.7287\n",
      "  train_runtime            = 13:43:07.18\n",
      "  train_samples_per_second =       0.576\n",
      "  train_steps_per_second   =       0.036\n"
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fe3d2e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3439\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='430' max='430' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [430/430 30:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        4.0\n",
      "  eval_accuracy           =       0.64\n",
      "  eval_f1                 =     0.6209\n",
      "  eval_loss               =     0.8002\n",
      "  eval_precision          =     0.6292\n",
      "  eval_recall             =       0.64\n",
      "  eval_runtime            = 0:30:25.79\n",
      "  eval_samples_per_second =      1.884\n",
      "  eval_steps_per_second   =      0.236\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate(prepared_ds['test'])\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
